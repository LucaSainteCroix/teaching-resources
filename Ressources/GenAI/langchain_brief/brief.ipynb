{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Créer un agent langchain capable de :\n",
    "* répondre à des questions sur les livres et leurs attributs (exemple : de quels sujets traite tel ou tel livre ?)\n",
    "* récupérer une liste des noms des personnages mentionnés dans le résumé du livre\n",
    "* si demandé par l'utilisateur, récupérer tout le texte d'un livre (.txt dispo sur https://gutenberg.org/) et répondre à des questions dessus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give me the python code for a FastAPI interface with these routes:\n",
    "\n",
    "- A main route which will decide which of the following routes to take based on the input text request, thanks to an LLM LangChain agent. \n",
    "\n",
    "- A route that takes in a natural language question about a book and returns a natural language answer after querying a RAG + LLM.\n",
    "- A route that handles specific questions about a book that could not be answered by the previous route. It will need to get the whole text of a book on such an url : https://gutenberg.org/cache/epub/74551/pg74551.txt , 74551 being the id of the book that is retrievable by the previous route. From that whole text it will use a RAG + LLM over it, to return a natural language answer.\n",
    "- A route that handles specific requests about what characters are mentioned in the summary of a book and returns a json list of characters.\n",
    "\n",
    "I don't need you to implement the RAGs or LLMs, I just need the FastAPI architecture and the Pydantic Schemas to define the inputs and outputs of the routes. You can divide the code in multiple files, like a FastAPI project would look like, if you think it's necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chaine de déroulement :\n",
    "\n",
    "- Appel à Agent Principal, qui a 3 tools associés, avec prompt définissant bien les possibilités\n",
    "- Cas 1 : `answer_question(question)` \n",
    "    - Il trouve la réponse, il renvoie la réponse\n",
    "    - Il ne trouve pas la réponse mais trouve le book_id, il appelle `answer_question_full_text(question, book_id)`\n",
    "    - Il ne trouve rien, il renvoie désolé je n'ai pas trouvé le livre en question.\n",
    "- Cas 2 : \n",
    "    - `answer_question_full_text(question, book_id)`\n",
    "    - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_brief_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
